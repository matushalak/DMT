{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Mac acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiParticipantDataset(Dataset):\n",
    "    def __init__(self, df, seq_length, target_col='target', id_col='id_num', include_target_in_features=True):\n",
    "        \"\"\"\n",
    "        df: pandas DataFrame sorted by time.\n",
    "        seq_length: number of time steps in each sample.\n",
    "        target_col: the column we want to predict.\n",
    "        \"\"\"\n",
    "        df = df.drop(columns=[\"next_date\", \"categorical_target\"])\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.target_col = target_col\n",
    "        self.id_col = id_col\n",
    "        \n",
    "        df.sort_values(by=[id_col, 'date', \"time_of_day\"], inplace=True) if \"time_of_day\" in df.columns else df.sort_values(by=[id_col, 'date'])\n",
    "        self.data = df.reset_index(drop=True)\n",
    "\n",
    "        if include_target_in_features:\n",
    "            self.features = [col for col in self.data.columns if col not in [target_col, \"date\"]]\n",
    "        else:\n",
    "            self.features = [col for col in self.data.columns if col not in [target_col, id_col, \"date\"]]\n",
    "\n",
    "        # Precompute valid indices where the sequence is within the same participant.\n",
    "        self.valid_indices = []\n",
    "        for i in range(len(self.data) - self.seq_length):\n",
    "            participant_id = self.data.iloc[i][self.id_col]\n",
    "            if all(self.data.iloc[i:i+self.seq_length][self.id_col] == participant_id):\n",
    "                self.valid_indices.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use precomputed valid index.\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        row = self.data.iloc[real_idx]\n",
    "        participant_id = row[self.id_col]\n",
    "        \n",
    "        x_features = self.data.iloc[real_idx:real_idx+self.seq_length][self.features].values.astype(np.float32)\n",
    "        x_id = np.array([participant_id]) # * self.seq_length, dtype=np.int64)\n",
    "        \n",
    "        # The target is the next time step's mood\n",
    "        y = self.data.iloc[real_idx+self.seq_length][self.target_col]\n",
    "        \n",
    "        return torch.tensor(x_features),torch.tensor(x_id), torch.tensor(y).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tables/imputed/df_ready_both.csv\")\n",
    "\n",
    "dataset_name = \"df_interpolated\"\n",
    "dropped_vars = [\"\"]\n",
    "imputation = \"linear interpolation\"\n",
    "\n",
    "# combine appCat cols\n",
    "app_cat = [col for col in df.columns if \"appCat\" in col]\n",
    "df[\"appCat\"] = df[app_cat].sum(axis=1)\n",
    "df.drop(columns=app_cat, inplace=True)\n",
    "df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = []\n",
    "dfs_test = []\n",
    "proportion_train = 0.8\n",
    "seq_length = 7\n",
    "\n",
    "for participant, group in df.groupby('id_num'):\n",
    "    group = group.sort_values(by='date')\n",
    "    # Calculate the split index based on the proportion\n",
    "    split_idx = int(len(group) * proportion_train)\n",
    "    # Ensure that the test set has at least seq_length + 1 samples\n",
    "    if len(group) - split_idx < seq_length + 1:\n",
    "        # Adjust the split index accordingly\n",
    "        split_idx = len(group) - (seq_length + 1)\n",
    "    if split_idx <= 0:\n",
    "        # Option: Skip this participant if not enough data\n",
    "        continue\n",
    "    dfs_train.append(group.iloc[:split_idx])\n",
    "    dfs_test.append(group.iloc[split_idx:])\n",
    "\n",
    "train_df = pd.concat(dfs_train)\n",
    "test_df = pd.concat(dfs_test)\n",
    "\n",
    "# SPLIT ACROSS PARTICIPANTS\n",
    "\n",
    "# # Extract unique participant IDs\n",
    "# participant_ids = df['id_num'].unique()\n",
    "\n",
    "# # Split participants (e.g., 80% train, 20% test)\n",
    "# train_ids, test_ids = train_test_split(participant_ids, test_size=0.2, random_state=42)\n",
    "# print(f\"Train IDs: {train_ids}\")\n",
    "# print(f\"Test IDs: {test_ids}\")\n",
    "\n",
    "# # Filter the original DataFrame based on these IDs\n",
    "# train_df = df[df['id_num'].isin(train_ids)].copy()\n",
    "# test_df = df[df['id_num'].isin(test_ids)].copy()\n",
    "\n",
    "# # Optional: sort your data by participant and day if not already sorted\n",
    "# train_df.sort_values(by=['id_num', 'day'], inplace=True)\n",
    "# test_df.sort_values(by=['id_num', 'day'], inplace=True)\n",
    "\n",
    "\n",
    "# get mood descriptives\n",
    "print(\"Train mood descriptives\")\n",
    "print(train_df[\"target\"].describe())\n",
    "print(\"Test mood descriptives\")\n",
    "print(test_df[\"target\"].describe())\n",
    "\n",
    "# get the start end end dates per participant per df\n",
    "train_start_dates = train_df.groupby('id_num')['date'].min()\n",
    "train_end_dates = train_df.groupby('id_num')['date'].max()\n",
    "test_start_dates = test_df.groupby('id_num')['date'].min()\n",
    "test_end_dates = test_df.groupby('id_num')['date'].max()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put in a dataframe with participant train start end and test start end\n",
    "dates_df = pd.DataFrame({\n",
    "    \"participant\": train_start_dates.index,\n",
    "    \"train_start\": train_start_dates.values,\n",
    "    \"train_end\": train_end_dates.values,\n",
    "    \"test_start\": test_start_dates.values,\n",
    "    \"test_end\": test_end_dates.values,\n",
    "})\n",
    "\n",
    "dates_df.to_csv(\"tables/training_dates_split.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_per_participant(df, scaler_dict=None, scaler_target_dict=None, transform_target=False, scaler_type=\"StandardScaler\", fit=True):\n",
    "    df = df.copy()\n",
    "    features = [col for col in df.columns if col not in ['id_num', \"date\", \"target\", \"next_date\"]]\n",
    "    \n",
    "    df_list = []\n",
    "    new_scaler_dict = {} if scaler_dict is None else scaler_dict.copy()\n",
    "    new_scaler_target_dict = {} if scaler_target_dict is None else scaler_target_dict.copy()\n",
    "\n",
    "    for pid, group in df.groupby(\"id_num\"):\n",
    "        group = group.copy()\n",
    "        \n",
    "        # Use existing or new scaler\n",
    "        if pid not in new_scaler_dict and fit:\n",
    "            scaler = StandardScaler() if scaler_type == \"StandardScaler\" else MinMaxScaler()\n",
    "            new_scaler_dict[pid] = scaler.fit(group[features])\n",
    "        scaler = new_scaler_dict[pid]\n",
    "        group[features] = scaler.transform(group[features])\n",
    "        \n",
    "        # Optionally transform target\n",
    "        if transform_target:\n",
    "            if pid not in new_scaler_target_dict and fit:\n",
    "                scaler_target = StandardScaler() if scaler_type == \"StandardScaler\" else MinMaxScaler()\n",
    "                new_scaler_target_dict[pid] = scaler_target.fit(group[[\"target\"]])\n",
    "            scaler_target = new_scaler_target_dict[pid]\n",
    "            group[\"target\"] = scaler_target.transform(group[[\"target\"]])\n",
    "        \n",
    "        df_list.append(group)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True), new_scaler_dict, new_scaler_target_dict\n",
    "\n",
    "# def denormalize(df, scaler, scaler_target=None):\n",
    "\n",
    "#     features = [col for col in df.columns if col not in ['id_num', 'day', \"date\", \"next_day_mood\", \"next_day\", \"mood\"]]\n",
    "    \n",
    "#     # Inverse transform the features\n",
    "#     df[features] = scaler.inverse_transform(df[features])\n",
    "    \n",
    "#     if scaler_target is not None:\n",
    "#         # Inverse transform the target column \"mood\"\n",
    "#         df[\"mood\"] = scaler_target.inverse_transform(df[[\"mood\"]])\n",
    "    \n",
    "    # return df\n",
    "\n",
    "TRANSFORM_TARGET = True\n",
    "SCALER_TYPE = \"MinMaxScaler\"\n",
    "# SCALER_TYPE = \"StandardScaler\"\n",
    "# Normalize the training and test data\n",
    "train_df_normalized, scaler, scaler_target = normalize_per_participant(train_df, transform_target=TRANSFORM_TARGET, scaler_type=SCALER_TYPE)\n",
    "test_df_normalized, _, _ = normalize_per_participant(test_df, scaler_dict=scaler, scaler_target_dict=scaler_target, fit=False, transform_target=TRANSFORM_TARGET, scaler_type=SCALER_TYPE)\n",
    "\n",
    "print(f\"Train shape: {train_df_normalized.shape}, Test shape: {test_df_normalized.shape}\") # Train shape: (1230, 24), Test shape: (307, 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the dataset\n",
    "seq_length = 7\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataset = MultiParticipantDataset(train_df_normalized, seq_length=seq_length)\n",
    "test_dataset = MultiParticipantDataset(test_df_normalized, seq_length=seq_length)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")\n",
    "for x, x_id, y in train_loader:\n",
    "    print(f\"x shape: {x.shape}, x_id shape: {x_id.shape}, y shape: {y.shape}\")\n",
    "    break\n",
    "# 32 sequences in a batch, each with 5 time steps and 24 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Two-layer LSTM with dropout applied to outputs of each layer (except the last)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully-connected layer to output the final prediction\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM; out shape: [batch_size, seq_length, hidden_dim]\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last time step's output for prediction; shape: [batch_size, hidden_dim]\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)  # shape: [batch_size, output_dim]\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Two-layer GRU with dropout applied between layers (if num_layers > 1)\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, \n",
    "                          batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully-connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate through GRU\n",
    "        out, _ = self.gru(x, h0)  # out shape: [batch_size, seq_length, hidden_dim]\n",
    "        \n",
    "        # Use the output from the last time step for prediction\n",
    "        out = out[:, -1, :]  # shape: [batch_size, hidden_dim]\n",
    "        out = self.fc(out)   # shape: [batch_size, output_dim]\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.2):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Multi-layer RNN with dropout applied between layers (if num_layers > 1)\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, \n",
    "                          batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully-connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, participant_ids=None, mask=None):\n",
    "        # x shape: [batch_size, seq_length, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Handle padding if mask is provided\n",
    "        if mask is not None:\n",
    "            # Calculate sequence lengths from mask\n",
    "            seq_lengths = mask.sum(dim=1).int()\n",
    "            \n",
    "            # Pack padded sequence\n",
    "            packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            \n",
    "            # Forward pass with packed sequence\n",
    "            packed_output, _ = self.rnn(packed_input, h0)\n",
    "            \n",
    "            # Unpack the sequence\n",
    "            out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            \n",
    "            # Extract the last valid output for each sequence\n",
    "            # Fix: Convert index tensor to Long (int64) type\n",
    "            idx = (seq_lengths - 1).view(-1, 1).unsqueeze(1).expand(-1, 1, self.hidden_dim).long()\n",
    "            last_out = out.gather(1, idx).squeeze(1)\n",
    "        else:\n",
    "            # Standard forward pass without packing\n",
    "            out, _ = self.rnn(x, h0)\n",
    "            # Use the last time step's output\n",
    "            last_out = out[:, -1, :]\n",
    "        \n",
    "        # Final prediction\n",
    "        out = self.fc(last_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "def train(config, train_loader, test_loader):\n",
    "    if torch.backends.mps.is_available():\n",
    "        mps_device = torch.device(\"mps\")\n",
    "    else:\n",
    "        mps_device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {mps_device}\")\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    # model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "    input_dim = config[\"input_dim\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    output_dim = config[\"output_dim\"]\n",
    "    dropout = config[\"dropout\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    model_type = config[\"model\"]\n",
    "\n",
    "    if model_type == \"LSTM\":\n",
    "        model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout)\n",
    "    elif model_type == \"RNN\":\n",
    "        model = SimpleRNNModel(input_dim, hidden_dim, num_layers, output_dim, dropout)\n",
    "    elif model_type == \"GRU\":\n",
    "        model = GRUModel(input_dim, hidden_dim, num_layers, output_dim, dropout)\n",
    "\n",
    "    model = model.to(mps_device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # weight_decay=1e-5)\n",
    "\n",
    "    # Prepare lists to store loss values for plotting\n",
    "    train_epoch_losses = []\n",
    "    eval_epoch_losses = []\n",
    "    batch_losses = []\n",
    "    grad_norms = []  # list to store gradient norms for each batch\n",
    "    clipped_grad_norms = []  # list to store clipped gradient norms for each batch\n",
    "\n",
    "\n",
    "    # Training and evaluation loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "\n",
    "        # --- Training ---\n",
    "        for batch in train_loader:\n",
    "            x_features, x_id, y = batch  # x_features: [batch, seq_length, input_dim]\n",
    "            x_features = x_features.to(mps_device)\n",
    "            x_id = x_id.to(mps_device)\n",
    "            y = y.to(mps_device)  # y: [batch, output_dim]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x_features)  # outputs shape: [batch, output_dim]\n",
    "            loss = criterion(outputs.squeeze(), y)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "            \n",
    "            # Backprop and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute total gradient norm\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            grad_norms.append(total_norm)\n",
    "            # Clip gradients\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # or another max_norm value\n",
    "\n",
    "            # Compute clipped gradient norm\n",
    "            clipped_grad_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    clipped_grad_norm += param_norm.item() ** 2\n",
    "            clipped_grad_norm = clipped_grad_norm ** 0.5\n",
    "            clipped_grad_norms.append(clipped_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "        train_epoch_losses.append(avg_train_loss)\n",
    "        \n",
    "        # --- Evaluation ---\n",
    "        model.eval()\n",
    "        eval_loss_epoch = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_features, x_id, y = batch\n",
    "                x_features = x_features.to(mps_device)\n",
    "                x_id = x_id.to(mps_device)\n",
    "                y = y.to(mps_device)\n",
    "                outputs = model(x_features)\n",
    "                loss = criterion(outputs.squeeze(), y)\n",
    "                eval_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_eval_loss = eval_loss_epoch / len(test_loader)\n",
    "        eval_epoch_losses.append(avg_eval_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "    # Plot the training and evaluation loss curves\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, num_epochs+1), train_epoch_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs+1), eval_epoch_losses, label='Eval Loss', marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.title(\"Training and Evaluation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot batch losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(batch_losses, label='Batch Loss')\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.title(\"Batch Loss Over Training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot gradient norms\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(grad_norms)\n",
    "    plt.plot(clipped_grad_norms)\n",
    "    plt.legend([\"Gradient Norm\", \"Clipped Gradient Norm\"])\n",
    "    plt.xlabel(\"Batch iteration\")\n",
    "    plt.ylabel(\"Gradient Norm (L2)\")\n",
    "    plt.title(\"Gradient Norms during Training\")\n",
    "    plt.show()\n",
    "\n",
    "    return model, train_epoch_losses, eval_epoch_losses, batch_losses, grad_norms, clipped_grad_norms\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"input_dim\": len(train_dataset.features),\n",
    "    \"hidden_dim\": 16,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_dim\": 1,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout\": 0.3,\n",
    "    \"model\": \"RNN\",\n",
    "}\n",
    "\n",
    "model, train_epoch_losses, eval_epoch_losses, batch_losses, grad_norms, clipped_grad_norms = train(config, train_loader, test_loader)\n",
    "\n",
    "\n",
    "# # append the csv with hyperparameters and losses\n",
    "# hyperparameters = {\n",
    "#     \"dataset\": dataset_name,\n",
    "#     \"dropped_vars\": dropped_vars,\n",
    "#     \"imputation\": imputation,\n",
    "#     \"model\": \"GRU\",\n",
    "#     \"sequence_length\": seq_length,\n",
    "#     \"scaler\": SCALER_TYPE,\n",
    "#     \"scaler_target\": TRANSFORM_TARGET,\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"input_dim\": input_dim,\n",
    "#     \"hidden_dim\": hidden_dim,\n",
    "#     \"num_layers\": num_layers,\n",
    "#     \"output_dim\": output_dim,\n",
    "#     \"dropout\": dropout,\n",
    "#     \"num_epochs\": num_epochs,\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"train_loss\": train_epoch_losses[-1],\n",
    "#     \"eval_loss\": eval_epoch_losses[-1],\n",
    "#     \"Features\": train_dataset.features,\n",
    "#     \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "# }\n",
    "# print(hyperparameters)\n",
    "# hyperparameters_df = pd.DataFrame([hyperparameters])\n",
    "# if not os.path.exists(\"tables/hyperparameters2.csv\"):\n",
    "#     # create the csv with the hyperparameters\n",
    "#     hyperparameters_df.to_csv(\"tables/hyperparameters2.csv\", index=False, header=True)\n",
    "# hyperparameters_df = pd.DataFrame([hyperparameters])\n",
    "# hyperparameters_df.to_csv(\"tables/hyperparameters2.csv\", mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot_simple(model, train_loader, test_loader, target_scaler=None, participant=None):\n",
    "    def get_predictions(loader):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        participant_ids = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                # Assuming your batch structure is (x, x_id, y)\n",
    "                x, x_id, y = batch  \n",
    "                x = x.cpu()\n",
    "                y = y.cpu()\n",
    "                # Convert x_id to CPU and then to numpy if needed for masking\n",
    "                x_id = x_id.cpu().numpy()\n",
    "                output = model(x)\n",
    "                predictions.append(output.cpu().numpy())  # Ensure output is on CPU\n",
    "                targets.append(y.cpu().numpy())\n",
    "                participant_ids.append(x_id)  # Collect participant IDs\n",
    "\n",
    "        predictions = np.concatenate(predictions).reshape(-1, 1)\n",
    "        targets = np.concatenate(targets).reshape(-1, 1)\n",
    "        participant_ids = np.concatenate(participant_ids).reshape(-1, 1)\n",
    "        # print(f\"Predictions shape: {predictions.shape}, Targets shape: {targets.shape}, Participant IDs shape: {participant_ids.shape}\")\n",
    "        # print(predictions)\n",
    "        # print(targets)\n",
    "        # print(participant_ids)\n",
    "\n",
    "        if target_scaler is not None:\n",
    "            # Loop through each participant and apply the respective inverse scaler\n",
    "            for participant, scaler in target_scaler.items():\n",
    "                # Create a boolean mask based on participant id (participant_ids is shape (n, 1))\n",
    "                mask = (participant_ids == participant)\n",
    "                # For predictions: reshape the 1D selection to 2D for the scaler, then flatten the result for assignment.\n",
    "                temp_pred = scaler.inverse_transform(predictions[mask].reshape(-1, 1))\n",
    "                predictions[mask] = temp_pred.ravel()\n",
    "                # Do the same for targets:\n",
    "                temp_tgt = scaler.inverse_transform(targets[mask].reshape(-1, 1))\n",
    "                targets[mask] = temp_tgt.ravel()\n",
    "            \n",
    "        return predictions.reshape(-1), targets.reshape(-1), participant_ids.reshape(-1)\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    y_pred_train, y_true_train, participant_ids = get_predictions(train_loader)\n",
    "    y_pred_test, y_true_test, participant_ids = get_predictions(test_loader)\n",
    "\n",
    "    def print_stats(y_true, y_pred, label=\"\"):\n",
    "        print(f\"\\n{label} Descriptive Statistics:\")\n",
    "        print(f\"Predictions - Mean: {np.mean(y_pred):.2f}, Std: {np.std(y_pred):.2f}, Min: {np.min(y_pred):.2f}, Max: {np.max(y_pred):.2f}\")\n",
    "        print(f\"Targets     - Mean: {np.mean(y_true):.2f}, Std: {np.std(y_true):.2f}, Min: {np.min(y_true):.2f}, Max: {np.max(y_true):.2f}\")\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"{label} MAE: {mae:.3f}, MSE: {mse:.3f}, R2: {r2:.3f}\")\n",
    "        return mae, mse, r2\n",
    "\n",
    "    # Print metrics\n",
    "    print_stats(y_true_train, y_pred_train, \"Train\")\n",
    "    print_stats(y_true_test, y_pred_test, \"Test\")\n",
    "\n",
    "    # select a specific participant\n",
    "    if participant is not None:\n",
    "        mask = (participant_ids == participant)\n",
    "        y_pred_participant = y_pred_test[mask]\n",
    "        y_true_participant = y_true_test[mask]\n",
    "        print(\"n data points in targets:\", len(y_true_participant))\n",
    "        print(\"n data points in predictions:\", len(y_pred_participant))\n",
    "        print_stats(y_true_participant, y_pred_participant, f\"Participant {participant}\")\n",
    "        # Plot for a specific participant\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(y_true_participant, label=\"Real\", alpha=0.7)\n",
    "        plt.plot(y_pred_participant, label=\"Predicted\", alpha=0.7)\n",
    "        plt.title(f\"Participant {participant}: Real vs Predicted\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylim(6,10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot results for training data\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(y_true_train, label=\"Real\", alpha=0.7)\n",
    "    plt.plot(y_pred_train, label=\"Predicted\", alpha=0.7)\n",
    "    plt.title(\"Train: Real vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot results for testing data\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(y_true_test, label=\"Real\", alpha=0.7)\n",
    "    plt.plot(y_pred_test, label=\"Predicted\", alpha=0.7)\n",
    "    plt.title(\"Test: Real vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    # Separate test plotting by participant\n",
    "    import math\n",
    "\n",
    "    unique_participants = np.unique(participant_ids)\n",
    "    n_parts = len(unique_participants)\n",
    "    # Determine grid size (roughly square)\n",
    "    n_cols = math.ceil(math.sqrt(n_parts))\n",
    "    n_rows = math.ceil(n_parts / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows), squeeze=False)\n",
    "    axes = axes.flatten()  # flatten to easily index each subplot\n",
    "\n",
    "    for idx, part in enumerate(unique_participants):\n",
    "        mask = (participant_ids == part)\n",
    "        y_true_part = y_true_test[mask]\n",
    "        y_pred_part = y_pred_test[mask]\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.plot(y_true_part, label=\"Real\", alpha=0.7)\n",
    "        ax.plot(y_pred_part, label=\"Predicted\", alpha=0.7)\n",
    "        # ax.set_title(f\"Real vs Predicted\")\n",
    "        ax.set_xlabel(f\"Participant {part}\")\n",
    "        ax.set_ylabel(\"Mood\")\n",
    "        ax.legend()\n",
    "        ax.set_ylim(4, 10)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Turn off any unused subplots.\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "predict_and_plot_simple(model, train_loader, test_loader, target_scaler=scaler_target, participant=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_loader, test_loader, train_dataset, output_dim, num_epochs):\n",
    "    \"\"\"\n",
    "    Performs grid search over specified hyperparameters including model type.\n",
    "    \n",
    "    Parameters:\n",
    "        train_loader : DataLoader for training data.\n",
    "        test_loader  : DataLoader for test/validation data.\n",
    "        train_dataset: Dataset used for training (to obtain input_dim).\n",
    "        output_dim   : Output dimension for the model.\n",
    "        num_epochs   : Number of epochs to train.\n",
    "    \n",
    "    Returns:\n",
    "        best_config: Dictionary with the best hyperparameters.\n",
    "        best_eval_loss: Final evaluation loss from the best config.\n",
    "        results: List with tuples (config, final_eval_loss) for each run.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define search grid for hyperparameters including the model type.\n",
    "    grid = {\n",
    "        \"hidden_dim\": [16],\n",
    "        \"num_layers\": [1, 2],\n",
    "        \"dropout\": [0.1, 0.2],\n",
    "        \"learning_rate\": [0.001],\n",
    "        \"model\": [\"GRU\", \"LSTM\", \"RNN\"]\n",
    "    }\n",
    "\n",
    "    \n",
    "    best_config = None\n",
    "    best_eval_loss = float('inf')\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over every combination of hyperparameters.\n",
    "    for hidden_dim in grid[\"hidden_dim\"]:\n",
    "        for num_layers in grid[\"num_layers\"]:\n",
    "            for dropout in grid[\"dropout\"]:\n",
    "                for learning_rate in grid[\"learning_rate\"]:\n",
    "                    for model_type in grid[\"model\"]:\n",
    "                        # Set up the configuration dictionary for the current combination.\n",
    "                        config = {\n",
    "                            \"input_dim\": len(train_dataset.features),\n",
    "                            \"hidden_dim\": hidden_dim,\n",
    "                            \"num_layers\": num_layers,\n",
    "                            \"output_dim\": output_dim,\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"dropout\": dropout,\n",
    "                            \"model\": model_type,\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"Training with config: {config}\")\n",
    "                        \n",
    "                        # Call the train function which returns the training and evaluation metrics.\n",
    "                        (model, train_epoch_losses, eval_epoch_losses,\n",
    "                         batch_losses, grad_norms, clipped_grad_norms) = train(config, train_loader, test_loader)\n",
    "                        \n",
    "                        # Use the final evaluation loss as the metric for grid search.\n",
    "                        final_eval_loss = eval_epoch_losses[-1]\n",
    "                        results.append((config, final_eval_loss))\n",
    "                        \n",
    "                        print(f\"Config: {config} -> Final Eval Loss: {final_eval_loss:.4f}\")\n",
    "                        \n",
    "                        # Update the best configuration if current one improves evaluation loss.\n",
    "                        if final_eval_loss < best_eval_loss:\n",
    "                            best_eval_loss = final_eval_loss\n",
    "                            best_config = config\n",
    "    \n",
    "    print(\"\\nGrid Search Complete!\")\n",
    "    print(f\"Best Config: {best_config}\")\n",
    "    print(f\"Best Final Eval Loss: {best_eval_loss:.4f}\")\n",
    "    return best_config, best_eval_loss, results\n",
    "\n",
    "best_config, best_loss, all_results = grid_search(train_loader, test_loader, train_dataset, output_dim, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
